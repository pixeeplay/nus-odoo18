import logging
import base64
import csv
from datetime import datetime
from pytz import timezone
from odoo import models, fields, api

_LOGGER = logging.getLogger(__name__)

class ImportAsyncHelper(models.TransientModel):
    _name = 'code2asin.import.async.helper'
    _description = 'Code2ASIN Import Async Helper'

    def process_import_async(self, config_record, import_session_id, log_model):
        """Traite l'import de mani√®re 'asynchrone' avec logs d√©taill√©s."""
        encoding_errors_count = 0
        
        try:
            # Log 3 : D√©but traitement
            log_model.create({
                'name': "Monitoring activ√© - traitement d√©marr√©",
                'log_type': 'info',
                'import_session_id': import_session_id
            })
            self.env.cr.commit()
            
            # Sauvegarder configuration
            config_record.save_config()
            
            # Log 4 : Chargement fichier
            log_model.create({
                'name': "Chargement et analyse du fichier CSV...",
                'log_type': 'info',
                'import_session_id': import_session_id
            })
            self.env.cr.commit()
            
            # D√©coder et analyser le fichier
            csv_data = base64.b64decode(config_record.csv_file)
            file_size_mb = len(csv_data) / (1024 * 1024)
            
            log_model.create({
                'name': f"Fichier charg√©: {file_size_mb:.1f} MB",
                'log_type': 'info',
                'import_session_id': import_session_id
            })
            self.env.cr.commit()
            
            csv_file_lines = config_record._parse_csv_with_encoding(csv_data)
            
            log_model.create({
                'name': f"D√©codage r√©ussi: {len(csv_file_lines)} lignes totales",
                'log_type': 'info',
                'import_session_id': import_session_id
            })
            self.env.cr.commit()
            
            # Parser le CSV avec protection
            reader = csv.reader(csv_file_lines, delimiter=',', quotechar='"')
            try:
                headers = next(reader, [])
                data_rows = list(reader)
            except Exception as e:
                log_model.create({
                    'name': f"ERREUR parsing CSV: {str(e)}",
                    'log_type': 'error',
                    'import_session_id': import_session_id
                })
                self.env.cr.commit()
                raise
            
            # Log en-t√™tes et donn√©es
            log_model.create({
                'name': f"En-t√™tes CSV: {', '.join(headers[:5])}{'...' if len(headers) > 5 else ''} (total: {len(headers)} colonnes)",
                'log_type': 'info',
                'import_session_id': import_session_id
            })
            self.env.cr.commit()
            
            log_model.create({
                'name': f"Donn√©es CSV: {len(data_rows)} lignes de produits √† traiter",
                'log_type': 'info',
                'import_session_id': import_session_id
            })
            self.env.cr.commit()
            
            # Cr√©er mapping
            mapping = config_record._map_csv_columns(headers)
            
            log_model.create({
                'name': f"Mapping cr√©√©: {str(mapping)}",
                'log_type': 'info',
                'import_session_id': import_session_id
            })
            self.env.cr.commit()
            
            # Traitement des lignes avec compteurs
            total_lines = len(data_rows)
            processed_count = 0
            created_count = 0
            updated_count = 0
            error_count = 0
            skipped_count = 0
            
            log_model.create({
                'name': f"D√âBUT DU TRAITEMENT DE {total_lines} PRODUITS",
                'log_type': 'info',
                'import_session_id': import_session_id
            })
            self.env.cr.commit()
            
            # Traitement ligne par ligne
            for row_index, row in enumerate(data_rows, start=2):  # Start=2 car ligne 1 = headers
                # V√©rifier si l'import doit s'arr√™ter
                is_running = self.env['ir.config_parameter'].sudo().get_param('code2asin.import_running', 'False')
                if is_running != 'True':
                    log_model.create({
                        'name': f"IMPORT ARR√äT√â PAR L'UTILISATEUR √† la ligne {row_index}",
                        'log_type': 'warning',
                        'import_session_id': import_session_id
                    })
                    break
                
                # Traiter cette ligne
                processor = self.env['code2asin.product.processor']
                result = processor.process_product_row(row, mapping, headers, row_index, import_session_id, log_model, config_record)
                
                # Compter les r√©sultats
                if result == 'created':
                    created_count += 1
                elif result == 'updated':
                    updated_count += 1
                elif result == 'error':
                    error_count += 1
                elif result == 'skipped':
                    skipped_count += 1
                
                processed_count += 1
                
                # Afficher progression tous les 10 produits
                if processed_count % 10 == 0:
                    progress_text = f"Progression: {processed_count}/{total_lines} ({(processed_count/total_lines)*100:.1f}%) - Cr√©√©s: {created_count}, Mis √† jour: {updated_count}, Erreurs: {error_count}, Ignor√©s: {skipped_count}"
                    log_model.create({
                        'name': progress_text,
                        'log_type': 'info',
                        'import_session_id': import_session_id
                    })
                    self.env.cr.commit()
            
            # R√©cap final complet avec toutes les statistiques
            success_rate = ((created_count + updated_count) / processed_count * 100) if processed_count > 0 else 0
            
            final_message = f"""=== IMPORT TERMIN√â AVEC SUCC√àS ===

üìä STATISTIQUES FINALES:
‚Ä¢ Lignes trait√©es: {processed_count}/{total_lines}
‚Ä¢ Nouveaux produits cr√©√©s: {created_count}
‚Ä¢ Produits mis √† jour: {updated_count}
‚Ä¢ Erreurs rencontr√©es: {error_count}
‚Ä¢ Lignes ignor√©es: {skipped_count}
‚Ä¢ Erreurs d'encodage d√©tect√©es: {encoding_errors_count}
‚Ä¢ Taux de succ√®s: {success_rate:.1f}%

‚úÖ L'import s'est termin√© normalement.
Session: {import_session_id}"""

            log_model.create({
                'name': final_message,
                'log_type': 'success' if error_count == 0 else 'warning',
                'import_session_id': import_session_id
            })
            self.env.cr.commit()
            
            # Log s√©par√© pour les erreurs d'encodage si n√©cessaire
            if encoding_errors_count > 0:
                log_model.create({
                    'name': f"‚ö†Ô∏è ATTENTION: {encoding_errors_count} produits avec des erreurs d'encodage d√©tect√©es dans les titres. V√©rifiez la qualit√© de votre fichier CSV source.",
                    'log_type': 'warning',
                    'import_session_id': import_session_id
                })
                self.env.cr.commit()
            
        except Exception as e:
            log_model.create({
                'name': f"‚ùå ERREUR IMPORT: {str(e)}",
                'log_type': 'error',
                'import_session_id': import_session_id
            })
            self.env.cr.commit()
            raise
            
        finally:
            # Marquer l'import comme termin√© TOUJOURS
            self.env['ir.config_parameter'].sudo().set_param('code2asin.import_running', 'False')
            self.env.cr.commit()
